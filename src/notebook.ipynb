{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from text_analysis import stem_sentence, remove_urls, remove_users, remove_retweets, TextCleaner\n",
    "from data import load_data, array_to_df\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "from scipy.sparse import lil_matrix, csr_matrix\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(\"labelled-tweets-20-09-2021.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_inputs(df, col):\n",
    "    tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "    tfidf = tfidf_vect.fit_transform(df[col])\n",
    "    unigram_counter = CountVectorizer(stop_words=stopwords.words(\"english\"), ngram_range=(1,1))\n",
    "    unigrams = unigram_counter.fit_transform(df[col])\n",
    "    bigram_counter = CountVectorizer(stop_words=stopwords.words(\"english\"), ngram_range=(2,2))\n",
    "    bigrams = bigram_counter.fit_transform(df[col])\n",
    "    unibigram_counter = CountVectorizer(stop_words=stopwords.words(\"english\"), ngram_range=(1,2))\n",
    "    unibigrams = unibigram_counter.fit_transform(df[col])\n",
    "    return tfidf, unigrams, bigrams, unibigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micdu\\Code\\microservices\\project-beacon\\tweets-analysis-service\\venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '00005',\n",
       " '0003',\n",
       " '002',\n",
       " '003',\n",
       " '005',\n",
       " '0097',\n",
       " '00am',\n",
       " '00michael02',\n",
       " '01',\n",
       " '018',\n",
       " '01_protocol',\n",
       " '01s',\n",
       " '02',\n",
       " '03',\n",
       " '04',\n",
       " '05',\n",
       " '050',\n",
       " '06',\n",
       " '06b',\n",
       " '07',\n",
       " '08',\n",
       " '085',\n",
       " '09',\n",
       " '0b',\n",
       " '0clckyp7zt',\n",
       " '0dpisxbivs',\n",
       " '0gtpqqoake',\n",
       " '0jfbi6gvti',\n",
       " '0kfpai4nre',\n",
       " '0mstnxgyfr',\n",
       " '0oocz3rrkt',\n",
       " '0ot35tcmw1',\n",
       " '0r6bbdp5ih',\n",
       " '0rz10qkqvr',\n",
       " '0v45vjmrpw',\n",
       " '0x00108',\n",
       " '0x4c756b65',\n",
       " '0x650d',\n",
       " '0x_clem',\n",
       " '0x_lucas',\n",
       " '0x_meow',\n",
       " '0x_tigerswami',\n",
       " '0xalena',\n",
       " '0xalice_',\n",
       " '0xaugustus',\n",
       " '0xb8adb3c41fb203a75f6952e91a8f26c83deaf2f0',\n",
       " '0xbebis_',\n",
       " '0xdazai',\n",
       " '0xdippur',\n",
       " '0xedenau',\n",
       " '0xgoober',\n",
       " '0xhalfinney',\n",
       " '0xminion',\n",
       " '0xmjs',\n",
       " '0xpolygon',\n",
       " '0xsisyphus',\n",
       " '0xthespaniard',\n",
       " '0xtuba',\n",
       " '0xwari',\n",
       " '0xwives',\n",
       " '0y6vcxsrj7',\n",
       " '0y8knkh9a1',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '10000',\n",
       " '100000',\n",
       " '1000x',\n",
       " '100k',\n",
       " '100m',\n",
       " '100x',\n",
       " '105',\n",
       " '106',\n",
       " '10k',\n",
       " '10m',\n",
       " '10no',\n",
       " '10x',\n",
       " '11',\n",
       " '112',\n",
       " '113',\n",
       " '114',\n",
       " '115',\n",
       " '118',\n",
       " '11am',\n",
       " '11dlepjyez',\n",
       " '11th',\n",
       " '12',\n",
       " '120mins',\n",
       " '127',\n",
       " '12am',\n",
       " '12noon',\n",
       " '12pm',\n",
       " '13',\n",
       " '130',\n",
       " '1350hrki8p',\n",
       " '13b',\n",
       " '13bn',\n",
       " '13th',\n",
       " '14',\n",
       " '140',\n",
       " '140m',\n",
       " '14b',\n",
       " '15',\n",
       " '150',\n",
       " '150k',\n",
       " '150x',\n",
       " '155',\n",
       " '1559',\n",
       " '157',\n",
       " '15m',\n",
       " '15p',\n",
       " '15th',\n",
       " '16',\n",
       " '166',\n",
       " '16th',\n",
       " '17',\n",
       " '170k',\n",
       " '17jpmujmya',\n",
       " '17th',\n",
       " '17x',\n",
       " '18',\n",
       " '180mins',\n",
       " '181',\n",
       " '185',\n",
       " '19',\n",
       " '1975',\n",
       " '1996',\n",
       " '1b',\n",
       " '1bn',\n",
       " '1ejz3rixum',\n",
       " '1f',\n",
       " '1goonrich',\n",
       " '1imuesxphs',\n",
       " '1inc',\n",
       " '1inch',\n",
       " '1inchcommunity',\n",
       " '1insdmfojr',\n",
       " '1k',\n",
       " '1m',\n",
       " '1million',\n",
       " '1nzmw4uj4w',\n",
       " '1pm',\n",
       " '1rdh6winqa',\n",
       " '1s',\n",
       " '1st',\n",
       " '1wjy71ihm8',\n",
       " '1x',\n",
       " '1yitnuzxbh',\n",
       " '1z0holx0z0',\n",
       " '1zz69evgbm',\n",
       " '20',\n",
       " '200',\n",
       " '2006',\n",
       " '200k',\n",
       " '200x',\n",
       " '2010',\n",
       " '2013',\n",
       " '2014',\n",
       " '2015',\n",
       " '2017',\n",
       " '2018',\n",
       " '2020',\n",
       " '2020s',\n",
       " '2021',\n",
       " '2022',\n",
       " '2028',\n",
       " '2031',\n",
       " '2035',\n",
       " '2042',\n",
       " '2051',\n",
       " '2061',\n",
       " '209',\n",
       " '20m',\n",
       " '20x',\n",
       " '21',\n",
       " '210929',\n",
       " '211005_release',\n",
       " '22',\n",
       " '2250',\n",
       " '2266nraufw',\n",
       " '22m',\n",
       " '23',\n",
       " '2300',\n",
       " '230php',\n",
       " '231ye7y94f',\n",
       " '23m',\n",
       " '23rd',\n",
       " '24',\n",
       " '240',\n",
       " '2400',\n",
       " '244',\n",
       " '24h',\n",
       " '24hrs',\n",
       " '24k',\n",
       " '24th',\n",
       " '24x',\n",
       " '25',\n",
       " '250',\n",
       " '252k',\n",
       " '25k',\n",
       " '25th',\n",
       " '25x',\n",
       " '26',\n",
       " '260',\n",
       " '262',\n",
       " '265',\n",
       " '269',\n",
       " '27',\n",
       " '28',\n",
       " '280',\n",
       " '288',\n",
       " '29',\n",
       " '292',\n",
       " '294',\n",
       " '29k',\n",
       " '29th',\n",
       " '2affqjrmgg',\n",
       " '2am',\n",
       " '2b',\n",
       " '2cdcm3xsff',\n",
       " '2e9jrlpci0',\n",
       " '2fafzazdb4',\n",
       " '2k',\n",
       " '2ldiacbtih',\n",
       " '2m',\n",
       " '2minqagow3',\n",
       " '2nd',\n",
       " '2pm',\n",
       " '2q',\n",
       " '2s',\n",
       " '2s6kju25w0',\n",
       " '2silyvrgki',\n",
       " '2t50bcgmte',\n",
       " '2tkiwk1q5q',\n",
       " '2tn',\n",
       " '2x',\n",
       " '2zqiathwqw',\n",
       " '30',\n",
       " '300',\n",
       " '300k',\n",
       " '306k',\n",
       " '309',\n",
       " '30pm',\n",
       " '30x',\n",
       " '31',\n",
       " '313',\n",
       " '315',\n",
       " '31st',\n",
       " '31th',\n",
       " '32',\n",
       " '326',\n",
       " '32k',\n",
       " '33',\n",
       " '35',\n",
       " '356g9csrm1',\n",
       " '357',\n",
       " '35x',\n",
       " '36',\n",
       " '360',\n",
       " '3675',\n",
       " '37',\n",
       " '371',\n",
       " '37300',\n",
       " '375',\n",
       " '38',\n",
       " '380',\n",
       " '380k',\n",
       " '38qt7glzir',\n",
       " '39',\n",
       " '390',\n",
       " '393',\n",
       " '3ac',\n",
       " '3bn',\n",
       " '3crv',\n",
       " '3d',\n",
       " '3g1wiu6kkm',\n",
       " '3l2iw9ik7v',\n",
       " '3l3c70',\n",
       " '3m',\n",
       " '3oemthbcc6',\n",
       " '3ply',\n",
       " '3pm',\n",
       " '3pool',\n",
       " '3q',\n",
       " '3scx0dxfbp',\n",
       " '3tquepwjr8',\n",
       " '3xckbj37mj',\n",
       " '40',\n",
       " '400',\n",
       " '400k',\n",
       " '400m',\n",
       " '40k',\n",
       " '40m',\n",
       " '41',\n",
       " '41rqrgq09u',\n",
       " '42',\n",
       " '42k',\n",
       " '43',\n",
       " '43b',\n",
       " '44k',\n",
       " '45',\n",
       " '46',\n",
       " '460',\n",
       " '4600',\n",
       " '46k',\n",
       " '46x',\n",
       " '47',\n",
       " '48',\n",
       " '48h',\n",
       " '48k',\n",
       " '49b',\n",
       " '4adssa',\n",
       " '4am',\n",
       " '4d4z2agskx',\n",
       " '4h',\n",
       " '4jt',\n",
       " '4m',\n",
       " '4p',\n",
       " '4pm',\n",
       " '4qerajste5',\n",
       " '4qrtbvkbyw',\n",
       " '4s',\n",
       " '4si4w7',\n",
       " '4srtddp9',\n",
       " '4th',\n",
       " '4wro4x8ghh',\n",
       " '4x',\n",
       " '50',\n",
       " '500',\n",
       " '500global',\n",
       " '500k',\n",
       " '500mil',\n",
       " '50k',\n",
       " '50m',\n",
       " '50pm',\n",
       " '524',\n",
       " '530',\n",
       " '5423',\n",
       " '55',\n",
       " '551',\n",
       " '56',\n",
       " '57m',\n",
       " '58',\n",
       " '58hkqlluso',\n",
       " '59pm',\n",
       " '5a09fqolxo',\n",
       " '5am',\n",
       " '5b',\n",
       " '5bn',\n",
       " '5djjzwf4fz',\n",
       " '5fanyvdy7o',\n",
       " '5fdbwtvplj',\n",
       " '5hdrjwrnf0',\n",
       " '5k',\n",
       " '5k0yrbg8qi',\n",
       " '5klq4mts2f',\n",
       " '5m',\n",
       " '5miibtky0b',\n",
       " '5mpdlv64xn',\n",
       " '5ntqmuzmqq',\n",
       " '5owa8r3l5d',\n",
       " '5pm',\n",
       " '5tn',\n",
       " '60',\n",
       " '600',\n",
       " '604',\n",
       " '608',\n",
       " '60k',\n",
       " '61',\n",
       " '622',\n",
       " '64',\n",
       " '644m',\n",
       " '645',\n",
       " '6485',\n",
       " '64gb',\n",
       " '64up1qchwr',\n",
       " '65',\n",
       " '65k',\n",
       " '68',\n",
       " '680',\n",
       " '69',\n",
       " '6axxw0njyr',\n",
       " '6b',\n",
       " '6bv9sr03qn',\n",
       " '6cdiaf7sge',\n",
       " '6iqsr2ca6r',\n",
       " '6jfyf3y8mr',\n",
       " '6kwe1yutq8',\n",
       " '6m',\n",
       " '6oahyvjumu',\n",
       " '6pm',\n",
       " '6pzeyiqbjs',\n",
       " '6s',\n",
       " '6uct8aheny',\n",
       " '6x',\n",
       " '6yvwi8xhpw',\n",
       " '70',\n",
       " '700',\n",
       " '7000',\n",
       " '70b',\n",
       " '70m',\n",
       " '72',\n",
       " '721k',\n",
       " '72lpqoa4yf',\n",
       " '73',\n",
       " '730',\n",
       " '74',\n",
       " '74pjmrqwqc',\n",
       " '75',\n",
       " '750',\n",
       " '7500',\n",
       " '75b',\n",
       " '777',\n",
       " '78kaitv8ep',\n",
       " '7b',\n",
       " '7boly5l4rh',\n",
       " '7eoj9sosdx',\n",
       " '7grz5djo1w',\n",
       " '7ioekodhsp',\n",
       " '7k',\n",
       " '7ky0ye1gim',\n",
       " '7l55pshogg',\n",
       " '7s5vqtksvu',\n",
       " '7sd14w67i1',\n",
       " '7ubhphjfbf',\n",
       " '7ulkqh5diy',\n",
       " '7xgyals8jh',\n",
       " '7xhuwynpag',\n",
       " '7xxp0cqeob',\n",
       " '80',\n",
       " '800',\n",
       " '8000',\n",
       " '81',\n",
       " '82b',\n",
       " '82k',\n",
       " '83tyhxgm2n',\n",
       " '84bkglhiie',\n",
       " '85',\n",
       " '850',\n",
       " '85mzm5r8dc',\n",
       " '85wqsblyhn',\n",
       " '87grej8xft',\n",
       " '88',\n",
       " '88mph',\n",
       " '88mphapp',\n",
       " '89',\n",
       " '899',\n",
       " '8a4ctzfhsy',\n",
       " '8b',\n",
       " '8f6hysx4gx',\n",
       " '8fp5litmal',\n",
       " '8i66ni1ubg',\n",
       " '8i8rfawvfs',\n",
       " '8kpuuy7vcd',\n",
       " '8kudaltdrt',\n",
       " '8m',\n",
       " '8tnux8o2is',\n",
       " '8vulk2ua3f',\n",
       " '90',\n",
       " '900',\n",
       " '9000',\n",
       " '90k',\n",
       " '90s',\n",
       " '91',\n",
       " '92',\n",
       " '92k',\n",
       " '9390',\n",
       " '93kxqzr3i4',\n",
       " '946',\n",
       " '95',\n",
       " '95xu5gksfc',\n",
       " '97',\n",
       " '979',\n",
       " '98',\n",
       " '99',\n",
       " '9amvgq6yf0',\n",
       " '9bfig0jjjt',\n",
       " '9fox0uddvs',\n",
       " '9gdf8pmdg4',\n",
       " '9ip4qotuxm',\n",
       " '9jzhjgzwwq',\n",
       " '9kme1lxelw',\n",
       " '9kmswbujcq',\n",
       " '9m',\n",
       " '9mi',\n",
       " '9nvi6kuxs2',\n",
       " '9pm',\n",
       " '9qmnashvsi',\n",
       " '9th',\n",
       " '9w9dlyihps',\n",
       " '9xbnm1dkyq',\n",
       " '9zjnhqnqe8',\n",
       " '_alchemistcoin',\n",
       " '_anishagnihotri',\n",
       " '_blakewest',\n",
       " '_crypto_bull',\n",
       " '_cryptosam_',\n",
       " '_dave__white_',\n",
       " '_doncryptoo',\n",
       " '_justathinker',\n",
       " '_kinjalbshah',\n",
       " '_mihirpatel',\n",
       " '_mo_malaka_',\n",
       " '_navdw',\n",
       " '_sadmx',\n",
       " '_selfmadedev',\n",
       " '_thechillguy',\n",
       " '_vshapovalov',\n",
       " 'a00vhajaxw',\n",
       " 'a10s',\n",
       " 'a16z',\n",
       " 'a2y7srbujr',\n",
       " 'a3cpxwuqvl',\n",
       " 'a3yebj1bww',\n",
       " 'a7l6armr3g',\n",
       " 'a9iyfyduaz',\n",
       " 'a9nn8bqmx4',\n",
       " 'a_cryptog',\n",
       " 'aaaannnd',\n",
       " 'aapl',\n",
       " 'aave',\n",
       " 'aaveaave',\n",
       " 'aavegrants',\n",
       " 'aavetokens',\n",
       " 'abcoathup',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'abl',\n",
       " 'able',\n",
       " 'abort',\n",
       " 'abortcontroller',\n",
       " 'abortion',\n",
       " 'abortions',\n",
       " 'abra',\n",
       " 'abracadabra',\n",
       " 'abroad',\n",
       " 'abs',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'abstracts',\n",
       " 'absurd',\n",
       " 'abu9ala7',\n",
       " 'abzhvfx7nw',\n",
       " 'acatlink',\n",
       " 'accelerate',\n",
       " 'accelerating',\n",
       " 'accelerator',\n",
       " 'accenture',\n",
       " 'accept',\n",
       " 'acceptance',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'access',\n",
       " 'accessible',\n",
       " 'accessing',\n",
       " 'accidentally',\n",
       " 'accomplished',\n",
       " 'according',\n",
       " 'account',\n",
       " 'accounts',\n",
       " 'accts',\n",
       " 'accumulate',\n",
       " 'accumulating',\n",
       " 'accumulation',\n",
       " 'accusation',\n",
       " 'accusing',\n",
       " 'accustomed',\n",
       " 'acfk7hvdyu',\n",
       " 'achieved',\n",
       " 'achievement',\n",
       " 'acknowledge',\n",
       " 'acn',\n",
       " 'acquire',\n",
       " 'acquired',\n",
       " 'acquisition',\n",
       " 'across',\n",
       " 'acryptoverse',\n",
       " 'action',\n",
       " 'actions',\n",
       " 'activate',\n",
       " 'active',\n",
       " 'activity',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'acumenofficial',\n",
       " 'ad',\n",
       " 'ada',\n",
       " 'add',\n",
       " 'added',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'additions',\n",
       " 'address',\n",
       " 'addressed',\n",
       " 'addresses',\n",
       " 'addyosmani',\n",
       " 'ade',\n",
       " 'adequate',\n",
       " 'administered',\n",
       " 'admire',\n",
       " 'admit',\n",
       " 'admitting',\n",
       " 'adopt',\n",
       " 'adoptio',\n",
       " 'adoption',\n",
       " 'adopts',\n",
       " 'adoptwombat',\n",
       " 'adouble212',\n",
       " 'adrian__chamber',\n",
       " 'adrian_trident',\n",
       " 'adrupsytov',\n",
       " 'ads',\n",
       " 'adult',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advantage',\n",
       " 'advice',\n",
       " 'adviser',\n",
       " 'advisor',\n",
       " 'advocate',\n",
       " 'aerospace',\n",
       " 'aespa',\n",
       " 'aesthetically',\n",
       " 'aeyakovenko',\n",
       " 'afaict',\n",
       " 'afaik',\n",
       " 'affairs',\n",
       " 'affected',\n",
       " 'affects',\n",
       " 'afiq_rsln',\n",
       " 'africa',\n",
       " 'african',\n",
       " 'afternoon',\n",
       " 'aftertilt',\n",
       " 'ag28cu88e2',\n",
       " 'agazdecki',\n",
       " 'age',\n",
       " 'agencies',\n",
       " 'agency',\n",
       " 'agenda',\n",
       " 'agent',\n",
       " 'aggregate',\n",
       " 'aggregator',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agreed',\n",
       " 'agreeing',\n",
       " 'agreement',\n",
       " 'agriculture',\n",
       " 'agrit_tiwari',\n",
       " 'ahead',\n",
       " 'ahh',\n",
       " 'ahhh',\n",
       " 'ahmad_alfalasi',\n",
       " 'ai',\n",
       " 'ai9lyjawdz',\n",
       " 'aide',\n",
       " 'aiding',\n",
       " 'aidonaks',\n",
       " 'aim',\n",
       " 'aiming',\n",
       " 'aims',\n",
       " 'aio7uuvdzm',\n",
       " 'air',\n",
       " 'airbnb',\n",
       " 'airdrop',\n",
       " 'airdropalert',\n",
       " 'airdropinspector',\n",
       " 'airdropped',\n",
       " 'airdrops',\n",
       " 'airdropstario',\n",
       " 'aires',\n",
       " 'airfare',\n",
       " 'aitbayevkanat',\n",
       " 'aj',\n",
       " 'aj6ydrj6fx',\n",
       " 'aja',\n",
       " 'ajgpo5rrwx',\n",
       " 'ajith',\n",
       " 'aka',\n",
       " 'akf9uqhwqs',\n",
       " 'akhirah',\n",
       " 'aknwqgnkt0',\n",
       " 'akropolis',\n",
       " 'akropolisio',\n",
       " 'aladdin',\n",
       " 'aladdindao',\n",
       " 'alameda',\n",
       " 'alamedatrabucco',\n",
       " 'albertwenger',\n",
       " 'album',\n",
       " 'alcaholicf',\n",
       " 'alchemix',\n",
       " 'alchemixfi',\n",
       " 'ald',\n",
       " 'aleenaaamir7',\n",
       " 'alejop_47',\n",
       " 'alena',\n",
       " 'alephile',\n",
       " 'alert',\n",
       " 'alexander_gr',\n",
       " 'alexanderdburch',\n",
       " 'alexanderfisher',\n",
       " 'alexandravbotez',\n",
       " 'alexbdebrie',\n",
       " 'alexberenson',\n",
       " 'alfa',\n",
       " 'algo',\n",
       " 'algorithmic',\n",
       " 'alhamdulillah',\n",
       " 'alibhamed',\n",
       " 'alienieee',\n",
       " 'allah',\n",
       " 'allard_jon',\n",
       " 'allbridge_io',\n",
       " 'allcoredevs',\n",
       " 'allegations',\n",
       " 'alleged',\n",
       " 'alley',\n",
       " 'allicoxyz',\n",
       " 'alliekmiller',\n",
       " 'alliestoken',\n",
       " 'allow',\n",
       " 'allowed',\n",
       " 'allowing',\n",
       " 'almost',\n",
       " 'along',\n",
       " 'alongside',\n",
       " 'alonzo',\n",
       " 'alpgasimov',\n",
       " 'alpha',\n",
       " 'alphaketchum',\n",
       " 'already',\n",
       " 'alright',\n",
       " 'also',\n",
       " 'alt',\n",
       " 'altair',\n",
       " 'altcoin',\n",
       " 'altcoingems',\n",
       " 'altcoingordon',\n",
       " 'altcoins',\n",
       " 'alternatives',\n",
       " 'although',\n",
       " 'altrank',\n",
       " 'alts',\n",
       " 'altseason',\n",
       " 'alusd',\n",
       " 'always',\n",
       " 'alzheimer',\n",
       " 'ama',\n",
       " 'amasad',\n",
       " 'amaz',\n",
       " 'amazing',\n",
       " 'amazon',\n",
       " 'ambitious',\n",
       " 'amc',\n",
       " 'amc_shareholder',\n",
       " 'america',\n",
       " 'american',\n",
       " 'amid',\n",
       " 'amirite',\n",
       " 'amm',\n",
       " 'ammunition',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amount',\n",
       " 'amounts',\n",
       " 'amp',\n",
       " 'amplify',\n",
       " 'ampt',\n",
       " 'amzn',\n",
       " 'ana',\n",
       " 'ana_andrianova',\n",
       " 'anakmenten6',\n",
       " 'analogy',\n",
       " 'analyses',\n",
       " 'analyst',\n",
       " 'ancestral',\n",
       " 'anderson',\n",
       " 'andre',\n",
       " 'andrecronjetech',\n",
       " 'andreessen',\n",
       " 'andrej_muzevic',\n",
       " 'andrewwarner',\n",
       " 'andriytyurnikov',\n",
       " 'android',\n",
       " 'andromeda',\n",
       " 'andy8052',\n",
       " 'anettrolikova',\n",
       " 'angle',\n",
       " 'angola_maldives',\n",
       " 'angry',\n",
       " 'angus',\n",
       " 'animal',\n",
       " 'animed',\n",
       " 'anitta',\n",
       " 'anlrr62bwn',\n",
       " 'anniversary',\n",
       " 'announce',\n",
       " 'announced',\n",
       " 'announcement',\n",
       " 'announces',\n",
       " 'announcing',\n",
       " 'annoying',\n",
       " 'annualized',\n",
       " 'anon',\n",
       " 'anons',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'answering',\n",
       " 'anthony',\n",
       " 'anti',\n",
       " 'antiprosynth',\n",
       " 'antitrust',\n",
       " 'anxiety',\n",
       " 'anybody',\n",
       " 'anyhow',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anyswapnetwork',\n",
       " 'anyt',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'aotmkyvelz',\n",
       " 'aouxt9nymu',\n",
       " 'ap',\n",
       " 'ape',\n",
       " 'ape9400',\n",
       " 'aped',\n",
       " 'apes',\n",
       " 'api',\n",
       " 'apifiny_',\n",
       " 'aplusk',\n",
       " 'apm3igpn2f',\n",
       " 'apologize',\n",
       " 'apologized',\n",
       " 'apompliano',\n",
       " 'apopoutsis',\n",
       " 'app',\n",
       " 'appages',\n",
       " 'apparently',\n",
       " 'appear',\n",
       " 'appearance',\n",
       " 'appears',\n",
       " 'apple',\n",
       " 'apples',\n",
       " 'application',\n",
       " 'applications',\n",
       " 'applied',\n",
       " 'apply',\n",
       " 'appraisal',\n",
       " 'appreciate',\n",
       " 'appreciated',\n",
       " 'approach',\n",
       " 'approaches',\n",
       " 'approve',\n",
       " 'approved',\n",
       " 'approx',\n",
       " 'apps',\n",
       " 'apqob1hqqj',\n",
       " 'apr',\n",
       " 'apricotfinance',\n",
       " 'april',\n",
       " 'aprs',\n",
       " 'apy',\n",
       " 'apys',\n",
       " 'aq8puaugkb',\n",
       " 'aqxu',\n",
       " 'aqzx6hwaro',\n",
       " 'ar',\n",
       " 'aragon',\n",
       " 'aramglitch',\n",
       " 'arashmarkazi',\n",
       " 'arbinyan',\n",
       " 'arbitrage',\n",
       " 'arbitrum',\n",
       " 'arc',\n",
       " 'arca',\n",
       " 'archaeologists',\n",
       " 'archaic',\n",
       " 'archetypes',\n",
       " 'archivos',\n",
       " 'area',\n",
       " 'areas',\n",
       " 'arent',\n",
       " 'argolinhasnft',\n",
       " 'argue',\n",
       " 'argued',\n",
       " 'argument',\n",
       " 'ariannasimpson',\n",
       " 'arisen',\n",
       " 'ark',\n",
       " 'arkansas',\n",
       " 'arkeon_vito',\n",
       " 'arkinvest',\n",
       " 'arm',\n",
       " 'armed',\n",
       " 'army',\n",
       " 'arno',\n",
       " 'arnotoken',\n",
       " 'around',\n",
       " 'arrive',\n",
       " 'arrived',\n",
       " 'arrives',\n",
       " 'arrowed',\n",
       " 'arrows',\n",
       " 'art',\n",
       " 'art_is_found',\n",
       " 'artblocks_io',\n",
       " 'artbynelly',\n",
       " 'artcdl',\n",
       " 'article',\n",
       " 'articles',\n",
       " 'artifact',\n",
       " 'artist',\n",
       " 'artists',\n",
       " 'arts',\n",
       " 'artwork',\n",
       " 'artworks',\n",
       " 'asf',\n",
       " 'ashaa_45',\n",
       " 'ask',\n",
       " 'askcryptowealth',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'asks',\n",
       " 'aspittel',\n",
       " 'ass',\n",
       " 'asse',\n",
       " 'assembled',\n",
       " 'asset',\n",
       " 'assets',\n",
       " 'asshole',\n",
       " 'assistance',\n",
       " 'assisting',\n",
       " 'associate',\n",
       " 'associated',\n",
       " 'association',\n",
       " 'assume',\n",
       " 'assuming',\n",
       " 'assumption',\n",
       " 'ast',\n",
       " 'astarnetwork',\n",
       " 'astrologycrypto',\n",
       " 'astronauts',\n",
       " 'astronomical',\n",
       " 'asus',\n",
       " 'asvanevik',\n",
       " 'aswell',\n",
       " 'ath',\n",
       " 'athletic',\n",
       " 'atlasonchain',\n",
       " 'atlee',\n",
       " 'atomicbags',\n",
       " 'atrixprotocol',\n",
       " 'att',\n",
       " 'attached',\n",
       " 'attack',\n",
       " 'attacker',\n",
       " 'attacking',\n",
       " 'attempt',\n",
       " 'attempting',\n",
       " 'attend',\n",
       " 'attendees',\n",
       " 'attention',\n",
       " 'attitude',\n",
       " 'attracted',\n",
       " 'attractive',\n",
       " 'attracts',\n",
       " 'auction',\n",
       " 'audience',\n",
       " 'audiences',\n",
       " 'audio',\n",
       " 'audit',\n",
       " 'auditable',\n",
       " 'auditors',\n",
       " 'audiusproject',\n",
       " 'aug',\n",
       " 'august',\n",
       " 'augusto',\n",
       " 'auqddsuspv',\n",
       " 'aurorian',\n",
       " 'auroryproject',\n",
       " 'auryn_macmillan',\n",
       " 'austerity_sucks',\n",
       " 'austingriffith',\n",
       " 'australia',\n",
       " 'austria',\n",
       " 'authentication',\n",
       " 'authority',\n",
       " 'authorized',\n",
       " 'authors',\n",
       " 'auto',\n",
       " 'automatically',\n",
       " 'autonomy',\n",
       " 'autoreward',\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_counter = CountVectorizer(stop_words=stopwords.words(\"english\"), ngram_range=(1,1))\n",
    "unigrams = unigram_counter.fit_transform(df[\"text\"])\n",
    "unigram_counter.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_raw, unigrams_raw, bigrams_raw, unibigrams_raw = prepare_inputs(df, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numerical_values(string):\n",
    "    return re.sub(\"\\\\d+(?=\\\\s|$)\", \"\", string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_clean\"] = df[\"text\"].apply(lambda text: remove_urls(text.lower()))\n",
    "df[\"text_clean\"] = df[\"text_clean\"].apply(remove_users)\n",
    "df[\"text_clean\"] = df[\"text_clean\"].apply(remove_retweets)\n",
    "df[\"text_clean\"] = df[\"text_clean\"].apply(remove_numerical_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '00am',\n",
       " '01',\n",
       " '01s',\n",
       " '02',\n",
       " '04',\n",
       " '050',\n",
       " '06b',\n",
       " '07',\n",
       " '08',\n",
       " '09',\n",
       " '0b',\n",
       " '0x650d',\n",
       " '0xb8adb3c41fb203a75f6952e91a8f26c83deaf2f',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '1000x',\n",
       " '100k',\n",
       " '100m',\n",
       " '100x',\n",
       " '105',\n",
       " '106',\n",
       " '10k',\n",
       " '10m',\n",
       " '10no',\n",
       " '10x',\n",
       " '11',\n",
       " '112',\n",
       " '113',\n",
       " '115',\n",
       " '118',\n",
       " '11am',\n",
       " '11th',\n",
       " '12',\n",
       " '120mins',\n",
       " '12am',\n",
       " '12noon',\n",
       " '12pm',\n",
       " '13',\n",
       " '130',\n",
       " '13b',\n",
       " '13bn',\n",
       " '13th',\n",
       " '14',\n",
       " '140',\n",
       " '140m',\n",
       " '14b',\n",
       " '15',\n",
       " '150k',\n",
       " '150x',\n",
       " '155',\n",
       " '15m',\n",
       " '15p',\n",
       " '15th',\n",
       " '16',\n",
       " '16th',\n",
       " '17',\n",
       " '170k',\n",
       " '17th',\n",
       " '17x',\n",
       " '18',\n",
       " '180mins',\n",
       " '185',\n",
       " '19',\n",
       " '1975',\n",
       " '1b',\n",
       " '1bn',\n",
       " '1f',\n",
       " '1inch',\n",
       " '1inchcommunity',\n",
       " '1k',\n",
       " '1m',\n",
       " '1million',\n",
       " '1pm',\n",
       " '1s',\n",
       " '1st',\n",
       " '1x',\n",
       " '20',\n",
       " '200',\n",
       " '200k',\n",
       " '200x',\n",
       " '2013',\n",
       " '2015',\n",
       " '2017',\n",
       " '2018',\n",
       " '2020',\n",
       " '2020s',\n",
       " '2021',\n",
       " '2022',\n",
       " '2035',\n",
       " '20m',\n",
       " '20x',\n",
       " '211005_release',\n",
       " '22',\n",
       " '23',\n",
       " '2300',\n",
       " '230php',\n",
       " '23rd',\n",
       " '24',\n",
       " '2400',\n",
       " '24h',\n",
       " '24hrs',\n",
       " '24k',\n",
       " '24th',\n",
       " '24x',\n",
       " '25',\n",
       " '252k',\n",
       " '25k',\n",
       " '25th',\n",
       " '25x',\n",
       " '26',\n",
       " '260',\n",
       " '262',\n",
       " '269',\n",
       " '27',\n",
       " '28',\n",
       " '280',\n",
       " '288',\n",
       " '29',\n",
       " '29k',\n",
       " '29th',\n",
       " '2am',\n",
       " '2b',\n",
       " '2k',\n",
       " '2m',\n",
       " '2nd',\n",
       " '2pm',\n",
       " '2q',\n",
       " '2s',\n",
       " '2tn',\n",
       " '2x',\n",
       " '30',\n",
       " '300',\n",
       " '300k',\n",
       " '306k',\n",
       " '30pm',\n",
       " '30x',\n",
       " '31',\n",
       " '31st',\n",
       " '31th',\n",
       " '32',\n",
       " '326',\n",
       " '32k',\n",
       " '33',\n",
       " '35',\n",
       " '357',\n",
       " '35x',\n",
       " '360',\n",
       " '37',\n",
       " '375',\n",
       " '38',\n",
       " '380',\n",
       " '380k',\n",
       " '39',\n",
       " '390',\n",
       " '393',\n",
       " '3ac',\n",
       " '3bn',\n",
       " '3d',\n",
       " '3m',\n",
       " '3ply',\n",
       " '3pm',\n",
       " '3pool',\n",
       " '3q',\n",
       " '40',\n",
       " '400',\n",
       " '400k',\n",
       " '400m',\n",
       " '40k',\n",
       " '40m',\n",
       " '42',\n",
       " '42k',\n",
       " '43',\n",
       " '43b',\n",
       " '44k',\n",
       " '46k',\n",
       " '46x',\n",
       " '47',\n",
       " '48',\n",
       " '48h',\n",
       " '48k',\n",
       " '49b',\n",
       " '4am',\n",
       " '4h',\n",
       " '4jt',\n",
       " '4m',\n",
       " '4p',\n",
       " '4pm',\n",
       " '4s',\n",
       " '4th',\n",
       " '4x',\n",
       " '50',\n",
       " '500',\n",
       " '500global',\n",
       " '500k',\n",
       " '500mil',\n",
       " '50k',\n",
       " '50m',\n",
       " '50pm',\n",
       " '524',\n",
       " '55',\n",
       " '56',\n",
       " '57m',\n",
       " '59pm',\n",
       " '5am',\n",
       " '5b',\n",
       " '5bn',\n",
       " '5k',\n",
       " '5m',\n",
       " '5pm',\n",
       " '5tn',\n",
       " '60',\n",
       " '604',\n",
       " '608',\n",
       " '60k',\n",
       " '64',\n",
       " '644m',\n",
       " '64gb',\n",
       " '65',\n",
       " '65k',\n",
       " '68',\n",
       " '680',\n",
       " '6b',\n",
       " '6m',\n",
       " '6pm',\n",
       " '6s',\n",
       " '6x',\n",
       " '70',\n",
       " '700',\n",
       " '70b',\n",
       " '70m',\n",
       " '721k',\n",
       " '73',\n",
       " '74',\n",
       " '75',\n",
       " '750',\n",
       " '75b',\n",
       " '7b',\n",
       " '7k',\n",
       " '80',\n",
       " '800',\n",
       " '81',\n",
       " '82b',\n",
       " '82k',\n",
       " '85',\n",
       " '850',\n",
       " '88',\n",
       " '88mph',\n",
       " '89',\n",
       " '8b',\n",
       " '8m',\n",
       " '9000',\n",
       " '90k',\n",
       " '90s',\n",
       " '91',\n",
       " '92',\n",
       " '92k',\n",
       " '95',\n",
       " '99',\n",
       " '9m',\n",
       " '9mi',\n",
       " '9pm',\n",
       " '9th',\n",
       " 'a10s',\n",
       " 'a16z',\n",
       " 'aaaannnd',\n",
       " 'aapl',\n",
       " 'aave',\n",
       " 'aavetokens',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'abl',\n",
       " 'able',\n",
       " 'abort',\n",
       " 'abortcontroller',\n",
       " 'abortions',\n",
       " 'abra',\n",
       " 'abracadabra',\n",
       " 'abroad',\n",
       " 'abs',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'abstracts',\n",
       " 'absurd',\n",
       " 'accelerate',\n",
       " 'accelerating',\n",
       " 'accelerator',\n",
       " 'accenture',\n",
       " 'accept',\n",
       " 'acceptance',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'access',\n",
       " 'accessible',\n",
       " 'accessing',\n",
       " 'accidentally',\n",
       " 'accomplished',\n",
       " 'according',\n",
       " 'account',\n",
       " 'accounts',\n",
       " 'accts',\n",
       " 'accumulate',\n",
       " 'accumulating',\n",
       " 'accumulation',\n",
       " 'accusation',\n",
       " 'accusing',\n",
       " 'accustomed',\n",
       " 'achieved',\n",
       " 'achievement',\n",
       " 'acknowledge',\n",
       " 'acn',\n",
       " 'acquire',\n",
       " 'acquired',\n",
       " 'acquisition',\n",
       " 'across',\n",
       " 'action',\n",
       " 'actions',\n",
       " 'activate',\n",
       " 'active',\n",
       " 'activity',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'ad',\n",
       " 'ada',\n",
       " 'add',\n",
       " 'added',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'additions',\n",
       " 'address',\n",
       " 'addressed',\n",
       " 'addresses',\n",
       " 'ade',\n",
       " 'adequate',\n",
       " 'administered',\n",
       " 'admire',\n",
       " 'admit',\n",
       " 'admitting',\n",
       " 'adopt',\n",
       " 'adoptio',\n",
       " 'adoption',\n",
       " 'adopts',\n",
       " 'ads',\n",
       " 'adult',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advantage',\n",
       " 'advice',\n",
       " 'adviser',\n",
       " 'advisor',\n",
       " 'advocate',\n",
       " 'aerospace',\n",
       " 'aespa',\n",
       " 'aesthetically',\n",
       " 'afaict',\n",
       " 'afaik',\n",
       " 'affairs',\n",
       " 'affected',\n",
       " 'affects',\n",
       " 'africa',\n",
       " 'african',\n",
       " 'afternoon',\n",
       " 'age',\n",
       " 'agencies',\n",
       " 'agency',\n",
       " 'agenda',\n",
       " 'agent',\n",
       " 'aggregate',\n",
       " 'aggregator',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agreed',\n",
       " 'agreeing',\n",
       " 'agreement',\n",
       " 'agriculture',\n",
       " 'ahead',\n",
       " 'ahh',\n",
       " 'ahhh',\n",
       " 'ai',\n",
       " 'aide',\n",
       " 'aiding',\n",
       " 'aim',\n",
       " 'aiming',\n",
       " 'aims',\n",
       " 'air',\n",
       " 'airbnb',\n",
       " 'airdrop',\n",
       " 'airdropalert',\n",
       " 'airdropinspector',\n",
       " 'airdropped',\n",
       " 'airdrops',\n",
       " 'aires',\n",
       " 'airfare',\n",
       " 'aj',\n",
       " 'aja',\n",
       " 'ajith',\n",
       " 'aka',\n",
       " 'akhirah',\n",
       " 'akropolis',\n",
       " 'aladdin',\n",
       " 'aladdindao',\n",
       " 'alameda',\n",
       " 'album',\n",
       " 'alchemix',\n",
       " 'ald',\n",
       " 'alena',\n",
       " 'alert',\n",
       " 'alfa',\n",
       " 'algo',\n",
       " 'algorithmic',\n",
       " 'alhamdulillah',\n",
       " 'allah',\n",
       " 'allcoredevs',\n",
       " 'allegations',\n",
       " 'alleged',\n",
       " 'alley',\n",
       " 'allow',\n",
       " 'allowed',\n",
       " 'allowing',\n",
       " 'almost',\n",
       " 'along',\n",
       " 'alongside',\n",
       " 'alonzo',\n",
       " 'alpha',\n",
       " 'already',\n",
       " 'alright',\n",
       " 'also',\n",
       " 'alt',\n",
       " 'altair',\n",
       " 'altcoin',\n",
       " 'altcoingems',\n",
       " 'altcoins',\n",
       " 'alternatives',\n",
       " 'although',\n",
       " 'altrank',\n",
       " 'alts',\n",
       " 'altseason',\n",
       " 'alusd',\n",
       " 'always',\n",
       " 'alzheimer',\n",
       " 'ama',\n",
       " 'amaz',\n",
       " 'amazing',\n",
       " 'amazon',\n",
       " 'ambitious',\n",
       " 'amc',\n",
       " 'america',\n",
       " 'american',\n",
       " 'amid',\n",
       " 'amirite',\n",
       " 'amm',\n",
       " 'ammunition',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amount',\n",
       " 'amounts',\n",
       " 'amp',\n",
       " 'amplify',\n",
       " 'ampt',\n",
       " 'amzn',\n",
       " 'ana',\n",
       " 'analogy',\n",
       " 'analyses',\n",
       " 'analyst',\n",
       " 'ancestral',\n",
       " 'anderson',\n",
       " 'andre',\n",
       " 'andreessen',\n",
       " 'android',\n",
       " 'andromeda',\n",
       " 'angle',\n",
       " 'angry',\n",
       " 'angus',\n",
       " 'animal',\n",
       " 'animed',\n",
       " 'anitta',\n",
       " 'anniversary',\n",
       " 'announce',\n",
       " 'announced',\n",
       " 'announcement',\n",
       " 'announces',\n",
       " 'announcing',\n",
       " 'annoying',\n",
       " 'annualized',\n",
       " 'anon',\n",
       " 'anons',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'anthony',\n",
       " 'anti',\n",
       " 'antitrust',\n",
       " 'anxiety',\n",
       " 'anybody',\n",
       " 'anyhow',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anyt',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'ap',\n",
       " 'ape',\n",
       " 'aped',\n",
       " 'apes',\n",
       " 'api',\n",
       " 'apologize',\n",
       " 'apologized',\n",
       " 'app',\n",
       " 'apparently',\n",
       " 'appear',\n",
       " 'appearance',\n",
       " 'appears',\n",
       " 'apple',\n",
       " 'apples',\n",
       " 'application',\n",
       " 'applications',\n",
       " 'applied',\n",
       " 'apply',\n",
       " 'appraisal',\n",
       " 'appreciate',\n",
       " 'appreciated',\n",
       " 'approach',\n",
       " 'approaches',\n",
       " 'approve',\n",
       " 'approved',\n",
       " 'approx',\n",
       " 'apps',\n",
       " 'apr',\n",
       " 'april',\n",
       " 'aprs',\n",
       " 'apy',\n",
       " 'apys',\n",
       " 'ar',\n",
       " 'arbitrage',\n",
       " 'arbitrum',\n",
       " 'arc',\n",
       " 'arca',\n",
       " 'archaeologists',\n",
       " 'archaic',\n",
       " 'archetypes',\n",
       " 'archivos',\n",
       " 'area',\n",
       " 'areas',\n",
       " 'arent',\n",
       " 'argue',\n",
       " 'argued',\n",
       " 'argument',\n",
       " 'arisen',\n",
       " 'ark',\n",
       " 'arkansas',\n",
       " 'arm',\n",
       " 'armed',\n",
       " 'army',\n",
       " 'arno',\n",
       " 'around',\n",
       " 'arrive',\n",
       " 'arrived',\n",
       " 'arrives',\n",
       " 'arrowed',\n",
       " 'arrows',\n",
       " 'art',\n",
       " 'article',\n",
       " 'articles',\n",
       " 'artifact',\n",
       " 'artist',\n",
       " 'artists',\n",
       " 'arts',\n",
       " 'artwork',\n",
       " 'artworks',\n",
       " 'asf',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'asks',\n",
       " 'ass',\n",
       " 'asse',\n",
       " 'assembled',\n",
       " 'asset',\n",
       " 'assets',\n",
       " 'asshole',\n",
       " 'assistance',\n",
       " 'assisting',\n",
       " 'associate',\n",
       " 'associated',\n",
       " 'association',\n",
       " 'assume',\n",
       " 'assuming',\n",
       " 'assumption',\n",
       " 'ast',\n",
       " 'astronauts',\n",
       " 'astronomical',\n",
       " 'asus',\n",
       " 'aswell',\n",
       " 'ath',\n",
       " 'athletic',\n",
       " 'atlee',\n",
       " 'att',\n",
       " 'attached',\n",
       " 'attack',\n",
       " 'attacker',\n",
       " 'attacking',\n",
       " 'attempt',\n",
       " 'attempting',\n",
       " 'attend',\n",
       " 'attendees',\n",
       " 'attention',\n",
       " 'attitude',\n",
       " 'attracted',\n",
       " 'attractive',\n",
       " 'attracts',\n",
       " 'auction',\n",
       " 'audience',\n",
       " 'audiences',\n",
       " 'audio',\n",
       " 'audit',\n",
       " 'auditable',\n",
       " 'auditors',\n",
       " 'aug',\n",
       " 'august',\n",
       " 'augusto',\n",
       " 'aurorian',\n",
       " 'australia',\n",
       " 'austria',\n",
       " 'authentication',\n",
       " 'authority',\n",
       " 'authorized',\n",
       " 'authors',\n",
       " 'auto',\n",
       " 'automatically',\n",
       " 'autonomy',\n",
       " 'autoreward',\n",
       " 'availability',\n",
       " 'available',\n",
       " 'avalanche',\n",
       " 'avant',\n",
       " 'avatar',\n",
       " 'avatars',\n",
       " 'avax',\n",
       " 'avent',\n",
       " 'average',\n",
       " 'avg',\n",
       " 'avid',\n",
       " 'aw',\n",
       " 'aware',\n",
       " 'awareness',\n",
       " 'away',\n",
       " 'awesome',\n",
       " 'awful',\n",
       " 'awkward',\n",
       " 'aws',\n",
       " 'axie',\n",
       " 'axl',\n",
       " 'axle',\n",
       " 'axs',\n",
       " 'b28demoday',\n",
       " 'ba',\n",
       " 'babybsc',\n",
       " 'babybunny',\n",
       " 'babysatoshi',\n",
       " 'bachelors',\n",
       " 'back',\n",
       " 'backed',\n",
       " 'backers',\n",
       " 'background',\n",
       " 'backwards',\n",
       " 'bad',\n",
       " 'badger',\n",
       " 'bag',\n",
       " 'bagging',\n",
       " 'bagholders',\n",
       " 'bags',\n",
       " 'balance',\n",
       " 'balancer',\n",
       " 'balk',\n",
       " 'ball',\n",
       " 'ban',\n",
       " 'banger',\n",
       " 'bank',\n",
       " 'banking',\n",
       " 'bankless',\n",
       " 'banknifty',\n",
       " 'banks',\n",
       " 'banksy',\n",
       " 'banned',\n",
       " 'banner',\n",
       " 'banning',\n",
       " 'bar',\n",
       " 'barely',\n",
       " 'barking',\n",
       " 'barrier',\n",
       " 'barriers',\n",
       " 'barrowland',\n",
       " 'base',\n",
       " 'based',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'basis',\n",
       " 'bastion',\n",
       " 'batch',\n",
       " 'bath',\n",
       " 'bathing',\n",
       " 'bathroom',\n",
       " 'batteries',\n",
       " 'battle',\n",
       " 'battlefield',\n",
       " 'başbelası',\n",
       " 'bb',\n",
       " 'bbbb',\n",
       " 'bc',\n",
       " 'bcf',\n",
       " 'bcs',\n",
       " 'bday',\n",
       " 'beach',\n",
       " 'beacon',\n",
       " 'bear',\n",
       " 'beard',\n",
       " 'bearing',\n",
       " 'bearish',\n",
       " 'bears',\n",
       " 'beast',\n",
       " 'beat',\n",
       " 'beatnikslovejpegs',\n",
       " 'beautiful',\n",
       " 'beauty',\n",
       " 'became',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'bed',\n",
       " 'beds',\n",
       " 'beef',\n",
       " 'beeple',\n",
       " 'beer',\n",
       " 'beerus',\n",
       " 'beforehand',\n",
       " 'begging',\n",
       " 'begin',\n",
       " 'beginning',\n",
       " 'begins',\n",
       " 'behalf',\n",
       " 'behe',\n",
       " 'behind',\n",
       " 'belami',\n",
       " 'believe',\n",
       " 'believes',\n",
       " 'belongs',\n",
       " 'benefit',\n",
       " 'benefits',\n",
       " 'benjamins2bitcoin',\n",
       " 'benzema',\n",
       " 'bergaransi',\n",
       " 'berkualitas',\n",
       " 'berlin',\n",
       " 'berliner',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'best',\n",
       " 'bet',\n",
       " 'beta',\n",
       " 'bets',\n",
       " 'better',\n",
       " 'bevy',\n",
       " 'bey',\n",
       " 'beyond',\n",
       " 'bias',\n",
       " 'bid',\n",
       " 'bidding',\n",
       " 'biden',\n",
       " 'bids',\n",
       " 'big',\n",
       " 'bigger',\n",
       " 'biggest',\n",
       " 'biking',\n",
       " 'bill',\n",
       " 'billion',\n",
       " 'billionaire',\n",
       " 'billions',\n",
       " 'binanc',\n",
       " 'binance',\n",
       " 'binancesmartchain',\n",
       " 'bio',\n",
       " 'biomarker',\n",
       " 'bipartisan',\n",
       " 'birdcage',\n",
       " 'birthday',\n",
       " 'bit',\n",
       " 'bitcoin',\n",
       " 'bitcoins',\n",
       " 'bitmart',\n",
       " 'bituman',\n",
       " 'bitumans',\n",
       " 'bizarre',\n",
       " 'black',\n",
       " 'blame',\n",
       " 'blank',\n",
       " 'blast',\n",
       " 'blatant',\n",
       " 'bless',\n",
       " 'blessed',\n",
       " 'blessedly',\n",
       " 'blessing',\n",
       " 'blind',\n",
       " 'blindly',\n",
       " 'bliss',\n",
       " 'bll',\n",
       " 'bln',\n",
       " 'block',\n",
       " 'blockchain',\n",
       " 'blockchains',\n",
       " 'blockchair',\n",
       " 'blocked',\n",
       " 'blocks',\n",
       " 'blog',\n",
       " 'blogger',\n",
       " 'bloggers',\n",
       " 'blow',\n",
       " 'blue',\n",
       " 'bluezilla',\n",
       " 'bnb',\n",
       " 'bo',\n",
       " 'board',\n",
       " 'boat',\n",
       " 'boatloads',\n",
       " 'bodily',\n",
       " 'body',\n",
       " 'boeing',\n",
       " 'bold',\n",
       " 'bond',\n",
       " 'bonds',\n",
       " 'bone',\n",
       " 'bonus',\n",
       " 'boo',\n",
       " 'book',\n",
       " 'bookings',\n",
       " 'books2read',\n",
       " 'boom',\n",
       " 'boomin',\n",
       " 'boosted',\n",
       " 'booster',\n",
       " 'boosting',\n",
       " 'bootstrapping',\n",
       " 'booty',\n",
       " 'bop',\n",
       " 'bored',\n",
       " 'boring',\n",
       " 'born',\n",
       " 'borrow',\n",
       " 'borrowers',\n",
       " 'borrowing',\n",
       " 'boss',\n",
       " 'bosses',\n",
       " 'bot',\n",
       " 'bother',\n",
       " 'bothered',\n",
       " 'bots',\n",
       " 'bottles',\n",
       " 'bottom',\n",
       " 'bottoms',\n",
       " 'bought',\n",
       " 'boule',\n",
       " 'bounce',\n",
       " 'bounced',\n",
       " 'boundaries',\n",
       " 'bounty',\n",
       " 'bowl',\n",
       " 'bowlers',\n",
       " 'box',\n",
       " 'boy',\n",
       " 'bpd',\n",
       " 'brag',\n",
       " 'brain',\n",
       " 'brakes',\n",
       " 'brand',\n",
       " 'brazil',\n",
       " 'brb',\n",
       " 'break',\n",
       " 'breakdown',\n",
       " 'breakfast',\n",
       " 'breaking',\n",
       " 'breaks',\n",
       " 'breathing',\n",
       " 'breathtaking',\n",
       " 'breed',\n",
       " 'brewing',\n",
       " 'bribed',\n",
       " 'bridge',\n",
       " 'bridges',\n",
       " 'bridging',\n",
       " 'brilliant',\n",
       " 'bring',\n",
       " 'bringing',\n",
       " 'britain',\n",
       " 'british',\n",
       " 'bro',\n",
       " 'broad',\n",
       " 'broadband',\n",
       " 'broader',\n",
       " 'broadway',\n",
       " 'brokerages',\n",
       " 'brotha',\n",
       " 'brother',\n",
       " 'brothers',\n",
       " 'brought',\n",
       " 'bru',\n",
       " 'bruh',\n",
       " 'bruins',\n",
       " 'bruno',\n",
       " 'brush',\n",
       " 'brushes',\n",
       " 'bsa',\n",
       " 'bsc',\n",
       " 'bscgems',\n",
       " 'bscpad',\n",
       " 'btc',\n",
       " 'bts',\n",
       " 'btw',\n",
       " 'bu',\n",
       " 'bubble',\n",
       " 'bubbling',\n",
       " 'bucks',\n",
       " 'budget',\n",
       " 'budweiser',\n",
       " 'buenos',\n",
       " 'buffalo',\n",
       " 'buffett',\n",
       " 'bug',\n",
       " 'bugs',\n",
       " 'build',\n",
       " 'builders',\n",
       " 'building',\n",
       " 'builds',\n",
       " 'built',\n",
       " 'bull',\n",
       " 'bullard',\n",
       " 'bullish',\n",
       " 'bullrun',\n",
       " 'bulls',\n",
       " 'bumper',\n",
       " 'bun',\n",
       " 'bunch',\n",
       " 'bunker',\n",
       " 'bunny',\n",
       " 'bureaucracy',\n",
       " 'burgers',\n",
       " 'burn',\n",
       " 'burned',\n",
       " 'burning',\n",
       " 'burry',\n",
       " 'bursin',\n",
       " 'busd',\n",
       " 'bushmeatbust',\n",
       " 'business',\n",
       " 'businesses',\n",
       " 'bust',\n",
       " 'busy',\n",
       " 'button',\n",
       " 'buy',\n",
       " 'buyback',\n",
       " 'buyer',\n",
       " 'buyers',\n",
       " 'buying',\n",
       " 'buyings',\n",
       " 'buys',\n",
       " 'bytecode',\n",
       " 'c0mm3nt',\n",
       " 'cadana',\n",
       " 'cai',\n",
       " 'cake',\n",
       " 'calendar',\n",
       " 'caliber',\n",
       " 'california',\n",
       " 'call',\n",
       " 'called',\n",
       " 'caller',\n",
       " 'calling',\n",
       " 'calls',\n",
       " 'calmly',\n",
       " 'came',\n",
       " 'camera',\n",
       " 'campaign',\n",
       " 'campaigns',\n",
       " 'candace',\n",
       " 'candidate',\n",
       " 'candle',\n",
       " 'candles',\n",
       " 'candy',\n",
       " 'cannot',\n",
       " 'cant',\n",
       " 'cap',\n",
       " 'capita',\n",
       " 'capital',\n",
       " 'capitalists',\n",
       " 'capitalize',\n",
       " 'capitalizes',\n",
       " 'capo',\n",
       " 'capped',\n",
       " 'capture',\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_counter_clean = CountVectorizer(stop_words=stopwords.words(\"english\"), ngram_range=(1,1))\n",
    "unigrams_clean = unigram_counter_clean.fit_transform(df[\"text_clean\"])\n",
    "unigram_counter_clean.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_clean, unigrams_clean, bigrams_clean, unibigrams_clean = prepare_inputs(df, \"text_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       join us discuss team understand certif legal p...\n",
       "1       onlin live learn platform hope train rise mana...\n",
       "2       congratul portfolio compani batch 25 rais 3m r...\n",
       "3                                     mani strong insight\n",
       "4       long set huberman lab podcast consider melaton...\n",
       "                              ...                        \n",
       "2118                                  rt rt offend someon\n",
       "2119    rt new wolf pattern stuff like mousepad shirt ...\n",
       "2120                           buck could buy new desktop\n",
       "2121           rt look crowd come sofi stadium today game\n",
       "2122    import observ would like add someth proactiv c...\n",
       "Name: text_stemmed, Length: 2123, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stem_sentences(string):\n",
    "    words = stem_sentence(string)\n",
    "    return \" \".join(words)\n",
    "\n",
    "df[\"text_stemmed\"] = df[\"text_clean\"].apply(stem_sentences)\n",
    "df[\"text_stemmed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    join us discuss team understand certif legal p...\n",
       "1    onlin live learn platform hope train rise mana...\n",
       "2    congratul portfolio compani batch 25 rais 3m r...\n",
       "3                                  mani strong insight\n",
       "4    long set huberman lab podcast consider melaton...\n",
       "5    go back life without anxieti know hotel withou...\n",
       "6    rt andreessen horowitz put massiv new crypto f...\n",
       "7                              huberman lab chang life\n",
       "8    horribl infuri unfair tragic sorri loss good r...\n",
       "9                            rt former cs love discuss\n",
       "Name: text_stemmed, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text_stemmed\"].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Join us for a discussion with the B-Lab team t...\n",
       "1     \"@newcampushq is an online, live learning plat...\n",
       "2     Congratulations to portfolio company, @Resonad...\n",
       "3     So many strong insights 👇👇 @FirstbaseHQ https:...\n",
       "4     @louisanicola_ Long set of Huberman Lab podcas...\n",
       "5     @ShaneMac @eightsleep Can’t go back to life wi...\n",
       "6     RT @BloombergTV: How Andreessen Horowitz is pu...\n",
       "7     @jenntejada Huberman Lab changed my life. @hub...\n",
       "8     @joelle_emerson Horrible. Infuriating, unfair,...\n",
       "9     RT @diarahmanTO: As a former CS, I am loving t...\n",
       "10    And we just had a last minute addition of the ...\n",
       "11    RT @kshenster: I'm discussing “Enterprise GTM:...\n",
       "12    The whole world of Customer Success is changin...\n",
       "13                    @btcarroccio You probably were!!!\n",
       "14                    @nealkhosla No disagreement here.\n",
       "15    We're not running a process, but we are talkin...\n",
       "16    RT @memdotai: Say goodbye to copy-paste.\\n\\nIn...\n",
       "17                      @SheilaSidhu This is great! 💯💯💯\n",
       "18    OH: “Twitter is LinkedIn for people who want t...\n",
       "19    GM to the bulls only\\n\\nApe World https://t.co...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @diarahmanTO: As a former CS, I am loving this discussion https://t.co/igyjB8CYZX'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"].loc[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micdu\\Code\\microservices\\project-beacon\\tweets-analysis-service\\venv\\lib\\site-packages\\pandas\\core\\indexing.py:1781: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>join us discuss team understand certif legal p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>onlin live learn platform hope train rise mana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>congratul portfolio compani batch 25 rais 3m r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mani strong insight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>long set huberman lab podcast consider melaton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>go back life without anxieti know hotel withou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>andreessen horowitz put massiv new crypto fund...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>huberman lab chang life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>horribl infuri unfair tragic sorri loss good r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>former cs love discuss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>last minut addit incred join us got 100x better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>discuss enterpris gtm custom success post sale...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>whole world custom success chang wildli world ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>probabl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>disagr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>run process talk select number firm 5 10 deck ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>say goodby introduc send via mem spotlight sav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>oh twitter linkedin peopl want lose lt amaz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gm bull ape world</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text\n",
       "0   join us discuss team understand certif legal p...\n",
       "1   onlin live learn platform hope train rise mana...\n",
       "2   congratul portfolio compani batch 25 rais 3m r...\n",
       "3                                 mani strong insight\n",
       "4   long set huberman lab podcast consider melaton...\n",
       "5   go back life without anxieti know hotel withou...\n",
       "6   andreessen horowitz put massiv new crypto fund...\n",
       "7                             huberman lab chang life\n",
       "8   horribl infuri unfair tragic sorri loss good r...\n",
       "9                              former cs love discuss\n",
       "10    last minut addit incred join us got 100x better\n",
       "11  discuss enterpris gtm custom success post sale...\n",
       "12  whole world custom success chang wildli world ...\n",
       "13                                            probabl\n",
       "14                                             disagr\n",
       "15  run process talk select number firm 5 10 deck ...\n",
       "16  say goodby introduc send via mem spotlight sav...\n",
       "17                                              great\n",
       "18        oh twitter linkedin peopl want lose lt amaz\n",
       "19                                  gm bull ape world"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess = Pipeline([('text_cleaner', TextCleaner())])\n",
    "preprocess.transform(df[[\"text\"]]).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_stem, unigrams_stem, bigrams_stem, unibigrams_stem = prepare_inputs(df, \"text_stemmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crypto</th>\n",
       "      <th>early stage</th>\n",
       "      <th>NFT</th>\n",
       "      <th>defi</th>\n",
       "      <th>dex</th>\n",
       "      <th>yield</th>\n",
       "      <th>lending</th>\n",
       "      <th>presale</th>\n",
       "      <th>oracles</th>\n",
       "      <th>giveaway</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2118</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2119</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2120</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2121</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2122</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2123 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      crypto  early stage    NFT   defi    dex  yield  lending  presale  \\\n",
       "0      False        False  False  False  False  False    False    False   \n",
       "1      False        False  False  False  False  False    False    False   \n",
       "2      False        False  False  False  False  False    False    False   \n",
       "3      False        False  False  False  False  False    False    False   \n",
       "4      False        False  False  False  False  False    False    False   \n",
       "...      ...          ...    ...    ...    ...    ...      ...      ...   \n",
       "2118   False        False  False  False  False  False    False    False   \n",
       "2119   False        False  False  False  False  False    False    False   \n",
       "2120   False        False  False  False  False  False    False    False   \n",
       "2121   False        False  False  False  False  False    False    False   \n",
       "2122   False        False  False  False  False  False    False    False   \n",
       "\n",
       "      oracles  giveaway  \n",
       "0       False     False  \n",
       "1       False     False  \n",
       "2       False     False  \n",
       "3       False     False  \n",
       "4       False     False  \n",
       "...       ...       ...  \n",
       "2118    False     False  \n",
       "2119    False     False  \n",
       "2120    False     False  \n",
       "2121    False     False  \n",
       "2122    False     False  \n",
       "\n",
       "[2123 rows x 10 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = array_to_df(df[\"topics\"])\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crypto         677\n",
       "early stage     36\n",
       "NFT            130\n",
       "defi           248\n",
       "dex             50\n",
       "yield           46\n",
       "lending         22\n",
       "presale          9\n",
       "oracles          2\n",
       "giveaway        69\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_cols = [\"crypto\", \"NFT\", \"defi\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_pipeline = Pipeline([\n",
    "    ('text_cleaner', TextCleaner(remove_urls=True)),\n",
    "    ('vectorizer', CountVectorizer(stop_words=stopwords.words(\"english\"))),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micdu\\Code\\microservices\\project-beacon\\tweets-analysis-service\\venv\\lib\\site-packages\\pandas\\core\\indexing.py:1781: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_pipeline.fit_transform(df[[\"text\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pipeline = Pipeline([\n",
    "    ('text_cleaner', TextCleaner(remove_urls=True)),\n",
    "    ('vectorizer', CountVectorizer(stop_words=stopwords.words(\"english\"))),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('classifier', SVC())\n",
    "])\n",
    "\n",
    "svc_search_params = {\n",
    "    \"text_cleaner__stem\": [True, False],\n",
    "    \"vectorizer__ngram_range\":[(1,1), (1,2), (2,2)],\n",
    "    \"classifier__kernel\":[\"linear\", \"rbf\"],\n",
    "    \"classifier__class_weight\": [\"balanced\", None]\n",
    "}\n",
    "\n",
    "svc_search = GridSearchCV(svc_pipeline, svc_search_params, cv=4, n_jobs=-1, scoring=\"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_pipeline = Pipeline([\n",
    "    ('text_cleaner', TextCleaner(remove_urls=True)),\n",
    "    ('vectorizer', CountVectorizer(stop_words=stopwords.words(\"english\"))),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('classifier', SGDClassifier())\n",
    "])\n",
    "\n",
    "sgd_search_params = {\n",
    "    \"text_cleaner__stem\": [True, False],\n",
    "    \"vectorizer__ngram_range\":[(1,1), (1,2), (2,2)],\n",
    "    \"classifier__kernel\":[\"linear\", \"rbf\"],\n",
    "}\n",
    "\n",
    "sgd_search = GridSearchCV(sgd_pipeline, sgd_search_params, cv=4, n_jobs=-1, scoring=\"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pipeline = Pipeline([\n",
    "    ('text_cleaner', TextCleaner(remove_urls=True)),\n",
    "    ('vectorizer', CountVectorizer(stop_words=stopwords.words(\"english\"))),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "nb_search_params = {\n",
    "    \"text_cleaner__stem\": [True, False],\n",
    "    \"vectorizer__ngram_range\":[(1,1), (1,2), (2,2)],\n",
    "}\n",
    "\n",
    "nb_search = GridSearchCV(nb_pipeline, nb_search_params, cv=4, n_jobs=-1, scoring=\"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "searches = [\n",
    "    (\"SVC\", svc_search), \n",
    "    #(\"SGD\", sgd_search), \n",
    "    (\"MNB\", nb_search)]\n",
    "\n",
    "def sort_by_recall(df):\n",
    "    return df.sort_values(\"recall\", ascending=False)\n",
    "\n",
    "def get_accuracy_precision_recall_f1(labels,pred):\n",
    "    acc = accuracy_score(labels,pred)\n",
    "    prec = precision_score(labels,pred,average='weighted') #,average='micro'\n",
    "    recal = recall_score(labels,pred,average='weighted')\n",
    "    f1 = f1_score(labels,pred,average='weighted')\n",
    "    return acc,prec,recal,f1\n",
    "\n",
    "def train_test_clf(x, y, clf):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n",
    "    print(\"train\",y_train)\n",
    "    print(\"test\",y_test)\n",
    "    clf.fit(x_train, y_train)\n",
    "    predictions = clf.predict(x_test)\n",
    "    accuracy, precision, recall, f1 = get_accuracy_precision_recall_f1(y_test, predictions)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def search_best(x, y):\n",
    "    results_df = pd.DataFrame(columns=[ \"classifier\", \"accuracy\", \"precision\", \"recall\", \"f1\"])\n",
    "    for name, search in searches:\n",
    "        print(\"Fitting\", name)\n",
    "        accuracy, precision, recall, f1 = train_test_clf(x, y, search)\n",
    "        results_df.loc[len(results_df.index)] = [name, accuracy, precision, recall, f1]\n",
    "        print(\"Best params:\", search.best_params_)\n",
    "    return sort_by_recall(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting SVC\n",
      "train       crypto\n",
      "1505   False\n",
      "516     True\n",
      "1461    True\n",
      "507     True\n",
      "815     True\n",
      "...      ...\n",
      "581    False\n",
      "1070   False\n",
      "1981   False\n",
      "288    False\n",
      "648    False\n",
      "\n",
      "[1592 rows x 1 columns]\n",
      "test       crypto\n",
      "189    False\n",
      "1952   False\n",
      "1694   False\n",
      "742    False\n",
      "1214    True\n",
      "...      ...\n",
      "1676   False\n",
      "1228   False\n",
      "290    False\n",
      "1574    True\n",
      "1139   False\n",
      "\n",
      "[531 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micdu\\Code\\microservices\\project-beacon\\tweets-analysis-service\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "96 fits failed out of a total of 96.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "64 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\micdu\\Code\\microservices\\project-beacon\\tweets-analysis-service\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\micdu\\Code\\microservices\\project-beacon\\tweets-analysis-service\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\micdu\\Code\\microservices\\project-beacon\\tweets-analysis-service\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 190, in fit\n",
      "    X, y = self._validate_data(\n",
      "  File \"C:\\Users\\micdu\\Code\\microservices\\project-beacon\\tweets-analysis-service\\venv\\lib\\site-packages\\sklearn\\base.py\", line 572, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\micdu\\Code\\microservices\\project-beacon\\tweets-analysis-service\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 973, in check_X_y\n",
      "    check_consistent_length(X, y)\n",
      "  File \"C:\\Users\\micdu\\Code\\microservices\\project-beacon\\tweets-analysis-service\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 331, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [1, 1194]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "32 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\micdu\\Code\\microservices\\project-beacon\\tweets-analysis-service\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\micdu\\Code\\microservices\\project-beacon\\tweets-analysis-service\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 390, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\micdu\\Code\\microservices\\project-beacon\\tweets-analysis-service\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 348, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\micdu\\Code\\microservices\\project-beacon\\tweets-analysis-service\\venv\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\micdu\\Code\\microservices\\project-beacon\\tweets-analysis-service\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 891, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\micdu\\Code\\microservices\\project-beacon\\tweets-analysis-service\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1321, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n",
      "  File \"C:\\Users\\micdu\\Code\\microservices\\project-beacon\\tweets-analysis-service\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1239, in _count_vocab\n",
      "    raise ValueError(\n",
      "ValueError: empty vocabulary; perhaps the documents only contain stop words\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\micdu\\Code\\microservices\\project-beacon\\tweets-analysis-service\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\micdu\\Code\\microservices\\project-beacon\\tweets-analysis-service\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:985: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1, 1592]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_28336/3332394406.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0msearch_best\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"text\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtopics\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"crypto\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_28336/2215042157.py\u001B[0m in \u001B[0;36msearch_best\u001B[1;34m(x, y)\u001B[0m\n\u001B[0;32m     27\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msearch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0msearches\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     28\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Fitting\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 29\u001B[1;33m         \u001B[0maccuracy\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mprecision\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrecall\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mf1\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain_test_clf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msearch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     30\u001B[0m         \u001B[0mresults_df\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresults_df\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maccuracy\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mprecision\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrecall\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mf1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     31\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Best params:\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msearch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbest_params_\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_28336/2215042157.py\u001B[0m in \u001B[0;36mtrain_test_clf\u001B[1;34m(x, y, clf)\u001B[0m\n\u001B[0;32m     18\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"train\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0my_train\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     19\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"test\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0my_test\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 20\u001B[1;33m     \u001B[0mclf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     21\u001B[0m     \u001B[0mpredictions\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mclf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_test\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     22\u001B[0m     \u001B[0maccuracy\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mprecision\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrecall\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mf1\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_accuracy_precision_recall_f1\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_test\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpredictions\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Code\\microservices\\project-beacon\\tweets-analysis-service\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, X, y, groups, **fit_params)\u001B[0m\n\u001B[0;32m    924\u001B[0m             \u001B[0mrefit_start_time\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    925\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0my\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 926\u001B[1;33m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbest_estimator_\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mfit_params\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    927\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    928\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbest_estimator_\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mfit_params\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Code\\microservices\\project-beacon\\tweets-analysis-service\\venv\\lib\\site-packages\\sklearn\\pipeline.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[0;32m    392\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_final_estimator\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[1;34m\"passthrough\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    393\u001B[0m                 \u001B[0mfit_params_last_step\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfit_params_steps\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msteps\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 394\u001B[1;33m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_final_estimator\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mXt\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mfit_params_last_step\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    395\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    396\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Code\\microservices\\project-beacon\\tweets-analysis-service\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    188\u001B[0m             \u001B[0mcheck_consistent_length\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    189\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 190\u001B[1;33m             X, y = self._validate_data(\n\u001B[0m\u001B[0;32m    191\u001B[0m                 \u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    192\u001B[0m                 \u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Code\\microservices\\project-beacon\\tweets-analysis-service\\venv\\lib\\site-packages\\sklearn\\base.py\u001B[0m in \u001B[0;36m_validate_data\u001B[1;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[0;32m    570\u001B[0m                 \u001B[0my\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcheck_array\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mcheck_y_params\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    571\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 572\u001B[1;33m                 \u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcheck_X_y\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mcheck_params\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    573\u001B[0m             \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    574\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Code\\microservices\\project-beacon\\tweets-analysis-service\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py\u001B[0m in \u001B[0;36mcheck_X_y\u001B[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001B[0m\n\u001B[0;32m    971\u001B[0m     \u001B[0my\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_check_y\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmulti_output\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmulti_output\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_numeric\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0my_numeric\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    972\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 973\u001B[1;33m     \u001B[0mcheck_consistent_length\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    974\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    975\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Code\\microservices\\project-beacon\\tweets-analysis-service\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py\u001B[0m in \u001B[0;36mcheck_consistent_length\u001B[1;34m(*arrays)\u001B[0m\n\u001B[0;32m    329\u001B[0m     \u001B[0muniques\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munique\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlengths\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    330\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0muniques\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 331\u001B[1;33m         raise ValueError(\n\u001B[0m\u001B[0;32m    332\u001B[0m             \u001B[1;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    333\u001B[0m             \u001B[1;33m%\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ml\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0ml\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mlengths\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: Found input variables with inconsistent numbers of samples: [1, 1592]"
     ]
    }
   ],
   "source": [
    "search_best(df[[\"text\"]], topics[[\"crypto\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics[\"crypto\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_clf(df[[\"text\"]], topics[[\"crypto\"]], svc_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics[[\"crypto\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_best(df[\"text\"], topics[\"defi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_best(df[\"text\"], topics[\"NFT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian, MLP and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_parameter_space = {\n",
    "    \"kernel\":[\"linear\", \"rbf\"]\n",
    "}\n",
    "\n",
    "def gscv_svc_factory():\n",
    "    svc_parameter_space = {\n",
    "        \"kernel\":[\"linear\", \"rbf\"],\n",
    "        \"class_weight\": [\"balanced\", None]\n",
    "    }\n",
    "    return GridSearchCV(SVC(), svc_parameter_space, n_jobs=-1, cv=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_clf(tfidf_raw, topics[\"crypto\"], gscv_svc_factory(), lambda cv: print(\"best params: \", cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_clf(unigrams_raw, topics[\"crypto\"], gscv_svc_factory(), lambda cv: print(\"best params: \", cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_clf(bigrams_raw, topics[\"crypto\"], gscv_svc_factory(), lambda cv: print(\"best params: \", cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_clf(unibigrams_raw, topics[\"crypto\"], gscv_svc_factory(), lambda cv: print(\"best params: \", cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_datasets = [\n",
    "    (tfidf_raw, \"tfidf_raw\"),\n",
    "    (unigrams_raw, \"unigrams_raw\"),\n",
    "    (bigrams_raw, \"bigrams_raw\"),\n",
    "    (unibigrams_raw, \"unibigrams_raw\"),\n",
    "    (tfidf_clean, \"tfidf_clean\"),\n",
    "    (unigrams_clean, \"unigrams_clean\"),\n",
    "    (bigrams_clean, \"bigrams_clean\"),\n",
    "    (unibigrams_clean, \"unibigrams_clean\"),\n",
    "    (tfidf_stem, \"tfidf_stem\"),\n",
    "    (unigrams_stem, \"unigrams_stem\"),\n",
    "    (bigrams_stem, \"bigrams_stem\"),\n",
    "    (unibigrams_stem, \"unibigrams_stem\"),\n",
    "]\n",
    "\n",
    "def train_test_repeat(x, y, clf_factory, n_iterations):\n",
    "    results_df = pd.DataFrame(columns=[ \"iteration\", \"accuracy\", \"precision\", \"recall\", \"f1\"])\n",
    "    for i in range(n_iterations):\n",
    "        results = train_test_clf(x, y, clf_factory())\n",
    "        results = np.asarray(results)\n",
    "        results = np.insert(results, 0, i)\n",
    "        results_df.loc[len(results_df.index)] = results\n",
    "    return results_df\n",
    "\n",
    "def train_test(clf_factory, datasets, col, dataset_transformer=False):\n",
    "    results_df = pd.DataFrame(columns=[\"dataset\", \"iteration\", \"accuracy\", \"precision\", \"recall\", \"f1\"])\n",
    "    for dataset, name in datasets:\n",
    "        data = dataset\n",
    "        if dataset_transformer:\n",
    "            data = dataset_transformer\n",
    "        train_test_df = train_test_repeat(data, topics[col], clf_factory, 5)\n",
    "        train_test_df[\"dataset\"] = name\n",
    "        results_df = results_df.append(train_test_df, ignore_index=True)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for SVC with different datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crypto topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svc_crypto_results = train_test(gscv_svc_factory, named_datasets, \"crypto\")\n",
    "svc_crypto_results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defi topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_defi_results = train_test(gscv_svc_factory, named_datasets, \"defi\")\n",
    "svc_crypto_results.sort_values(\"recall\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NFT topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_nft_results = train_test(gscv_svc_factory, named_datasets, \"NFT\")\n",
    "svc_crypto_results.sort_values(\"recall\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "def gscv_gnb_factory():\n",
    "    gnb_parameter_space = {\n",
    "    }\n",
    "    return GridSearchCV(GaussianNB(), gnb_parameter_space, n_jobs=-1, cv=2)\n",
    "\n",
    "train_test_clf(words, topics[\"crypto\"], gscv_gnb_factory())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_recall(df):\n",
    "    return df.sort_values(\"recall\", ascending=False)\n",
    "\n",
    "n_dimensions = [2500, 1000, 500, 200, 100, 50, 20]\n",
    "def train_test_reduced_dimensions(clf_factory, datasets, col, n_components):\n",
    "    results_df = pd.DataFrame(columns=[\"n_components\", \"dataset\", \"iteration\", \"accuracy\", \"precision\", \"recall\", \"f1\"])\n",
    "    train_df = train_test(gscv_svc_factory, named_datasets, \"crypto\")\n",
    "    train_df[\"n_components\"] = \"-1\"\n",
    "    results_df = results_df.append(train_df, ignore_index=True)\n",
    "    for dataset, name in datasets:\n",
    "        for n in n_components:\n",
    "            svd = TruncatedSVD(n_components=n, n_iter=7)\n",
    "            x = svd.fit_transform(dataset)\n",
    "            train_df = train_test_repeat(x, topics[col], clf_factory, 5)\n",
    "            train_df[\"dataset\"] = name\n",
    "            train_df[\"n_components\"] = n\n",
    "            results_df = results_df.append(train_df, ignore_index=True)\n",
    "    return sort_by_recall(results_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crypto topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_results_crypto_dimensions = train_test_reduced_dimensions(gscv_svc_factory, named_datasets, \"crypto\", n_dimensions)\n",
    "train_results_crypto_dimensions.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results_crypto_dimensions_gnb = train_test_reduced_dimensions(gscv_gnb_factory, named_datasets, \"crypto\", n_dimensions)\n",
    "train_results_crypto_dimensions_gnb.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "def ovsr_svc_gnb_factory():\n",
    "    parameter_space = {\n",
    "    }\n",
    "    return GridSearchCV(OneVsRestClassifier(SVC(kernel=\"linear\")), parameter_space, n_jobs=-1, cv=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results_crypto_dimensions_gnb = train_test_reduced_dimensions(ovsr_svc_gnb_factory, named_datasets, [\"crypto\", \"NFT\", \"defi\"], n_dimensions)\n",
    "train_results_crypto_dimensions_gnb.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NFT topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_results_nft_dimensions_svc = train_test_reduced_dimensions(gscv_svc_factory, named_datasets, \"NFT\", n_dimensions)\n",
    "train_results_nft_dimensions_svc.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defi topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_results_defi_dimensions_svc = train_test_reduced_dimensions(gscv_svc_factory, named_datasets, \"defi\", n_dimensions)\n",
    "train_results_defi_dimensions_svc.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase are marginals, but reducing dimensionnality to 100-200 seems to yield better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building final models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = \"models\"\n",
    "\n",
    "def train_and_save(topic ,vectorizer, dimension_reductor, model):\n",
    "    dataset = vectorizer.fit_transform(df[\"text\"])\n",
    "    save_model(vectorizer, \"vectorizer\", topic)\n",
    "    dataset = dimension_reductor.fit_transform(dataset)\n",
    "    save_model(dimension_reductor, \"reducer\", topic)\n",
    "    model.fit(dataset, topics[topic])\n",
    "    save_model(model, \"model\", topic)\n",
    "\n",
    "def save_model(model, role, topic):\n",
    "    filename = get_filename(role, topic)\n",
    "    folder = save_folder\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    filepath = folder +\"/\"+filename\n",
    "    with open(filepath, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "\n",
    "def get_filename(role, topic):\n",
    "    return role+\"-\"+topic+\".pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_save(\n",
    "    \"crypto\", \n",
    "    CountVectorizer(stop_words=stopwords.words(\"english\"), ngram_range=(1,1)),\n",
    "    TruncatedSVD(n_components=100, n_iter=7),\n",
    "    SVC(kernel=\"linear\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_save(\n",
    "    \"NFT\", \n",
    "    CountVectorizer(stop_words=stopwords.words(\"english\"), ngram_range=(1,1)),\n",
    "    TruncatedSVD(n_components=100, n_iter=7),\n",
    "    SVC(kernel=\"linear\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_save(\n",
    "    \"defi\", \n",
    "    CountVectorizer(stop_words=stopwords.words(\"english\"), ngram_range=(1,1)),\n",
    "    TruncatedSVD(n_components=200, n_iter=7),\n",
    "    SVC(kernel=\"linear\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "save_folder = \"models\"\n",
    "\n",
    "def get_filename(role, topic):\n",
    "    return role + \"-\" + topic + \".pkl\"\n",
    "\n",
    "def load_pipeline(label):\n",
    "    vectorizer = load_pipe(\"vectorizer\", label)\n",
    "    reducer = load_pipe(\"reducer\", label)\n",
    "    model = load_pipe(\"model\", label)\n",
    "    return Pipeline([vectorizer, reducer, model])\n",
    "    \n",
    "def load_pipe(role, label):\n",
    "    return (role, load_model(role, label))\n",
    "    \n",
    "def load_model(role, label):\n",
    "    filename = get_filename(role, label)\n",
    "    folder = save_folder\n",
    "    filepath = folder +\"/\"+filename\n",
    "    model = None\n",
    "    with open(filepath, 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto_labeller = load_pipeline(\"crypto\")\n",
    "predictions_crypto = crypto_labeller.predict(unigrams_raw)\n",
    "get_accuracy_precision_recall_f1(y_test, predictions_crypto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defi_labeller = load_pipeline(\"defi\")\n",
    "defi_labeller.predict([\n",
    "    \"This is a tweet about Bitcoin\", \n",
    "    \"Check out this defi project\", \n",
    "    \"lending is insane\", \n",
    "    \"on ethereum\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nft_labeller = load_pipeline(\"NFT\")\n",
    "nft_labeller.predict([\n",
    "    \"This is a tweet about Bitcoin\", \n",
    "    \"Check out this defi project\", \n",
    "    \"I love NFTs\", \n",
    "    \"I bought this cryptopunk\",\n",
    "    \"rarity\"\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}